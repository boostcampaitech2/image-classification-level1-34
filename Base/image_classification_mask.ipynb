{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 마스크 착용 상태 분류\n",
    "(Aistages Competition)  \n",
    "카메라로 촬영한 사람 얼굴 이미지의 마스크 착용여부를 판단하는 Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 대회 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. 배경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/about_competition.png\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. 평가방법 (Metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/about_competition2.png\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. 데이터 셋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/sample_image.png\" width=600px/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전체 사람 수 : 4,500명  \n",
    "- 한 사람 당 사진 갯수 : 7개 (마스크 착용 5, 미착용 1, 잘못착용 1)  \n",
    "- 이미지 크기 : (384, 512)  \n",
    "- 결과 클래스 (Label)  \n",
    "\n",
    "<img src=\"./images/label.png\" width=600px>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/flow.png\" width=600px>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 이해\n",
    "- Dataset 생성\n",
    "- Model(Module) 생성\n",
    "- Train (학습)\n",
    "- 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1. 데이터 이해 (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA과정으로 주어진 데이터의 수치나 분포등을 통해 이해하는 단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보다 다양한 EDA데이터는 강의를 통해 얻은 './sample_code/2_EDA.ipynb' 파일을 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/opt/ml/input/data/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  gender   race  age                    path\n",
       "0  000001  female  Asian   45  000001_female_Asian_45\n",
       "1  000002  female  Asian   52  000002_female_Asian_52\n",
       "2  000004    male  Asian   54    000004_male_Asian_54\n",
       "3  000005  female  Asian   58  000005_female_Asian_58\n",
       "4  000006  female  Asian   59  000006_female_Asian_59"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7eff63c95ca0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 나이 그래프\n",
    "df.age.value_counts().sort_index().plot(kind='bar', figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~30        1281\n",
      "30 ~ 60    1227\n",
      "60 ~        192\n",
      "Name: age_range, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7eff6386df70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['age_range'] = df['age'].apply(lambda x: '~30' if x < 30 else ('60 ~' if x >= 60 else '30 ~ 60'))\n",
    "print(df.age_range.value_counts())\n",
    "df.age_range.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "      <th>age_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>001063</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>60</td>\n",
       "      <td>001063_female_Asian_60</td>\n",
       "      <td>60 ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>001102</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>60</td>\n",
       "      <td>001102_female_Asian_60</td>\n",
       "      <td>60 ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>001106</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>60</td>\n",
       "      <td>001106_female_Asian_60</td>\n",
       "      <td>60 ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>001172</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>60</td>\n",
       "      <td>001172_female_Asian_60</td>\n",
       "      <td>60 ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>001363</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>60</td>\n",
       "      <td>001363_female_Asian_60</td>\n",
       "      <td>60 ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>005446</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>60</td>\n",
       "      <td>005446_female_Asian_60</td>\n",
       "      <td>60 ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>005453</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>60</td>\n",
       "      <td>005453_female_Asian_60</td>\n",
       "      <td>60 ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2175</th>\n",
       "      <td>005461</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>60</td>\n",
       "      <td>005461_female_Asian_60</td>\n",
       "      <td>60 ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>005504</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>60</td>\n",
       "      <td>005504_female_Asian_60</td>\n",
       "      <td>60 ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>005515</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>60</td>\n",
       "      <td>005515_female_Asian_60</td>\n",
       "      <td>60 ~</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  gender   race  age                    path age_range\n",
       "443   001063  female  Asian   60  001063_female_Asian_60      60 ~\n",
       "477   001102  female  Asian   60  001102_female_Asian_60      60 ~\n",
       "480   001106  female  Asian   60  001106_female_Asian_60      60 ~\n",
       "540   001172  female  Asian   60  001172_female_Asian_60      60 ~\n",
       "671   001363  female  Asian   60  001363_female_Asian_60      60 ~\n",
       "...      ...     ...    ...  ...                     ...       ...\n",
       "2161  005446  female  Asian   60  005446_female_Asian_60      60 ~\n",
       "2168  005453  female  Asian   60  005453_female_Asian_60      60 ~\n",
       "2175  005461  female  Asian   60  005461_female_Asian_60      60 ~\n",
       "2209  005504  female  Asian   60  005504_female_Asian_60      60 ~\n",
       "2217  005515  female  Asian   60  005515_female_Asian_60      60 ~\n",
       "\n",
       "[109 rows x 6 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나이 그래프\n",
    "#print(df.gender.value_counts())\n",
    "#df.loc[df['gender'] == 'female']['age_range'].value_counts()\n",
    "#print('------------------------------------------------------')\n",
    "#grouped = df.groupby(level=['gender', 'age_range'])\n",
    "#print(df.loc[df['gender'] == 'female']['age_range'].value_counts())\n",
    "#print(df.loc[df['gender'] == 'male']['age_range'].value_counts())\n",
    "df.loc[df['gender'] == 'female'].loc[df['age_range'] == '60 ~']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 과정을 통해 클래스 (라벨)을 분류하는데 기준이 되는 나이 분포가 고르지 않음을 알 수 있었다.  \n",
    "  => 60세의 데이터가 비교적 평균보다 많이 있지만 60세 이상의 나이가 60세만 있어 데이터의 불균형을 볼 수 있었다.  \n",
    "  => 30~60 세의 데이터가 매우 넓게 분포되어 있음을 볼 수 있었다.\n",
    "- sample_code를 보며 마스크의 색상이 단색인 경우가 대부분이다 보니 특정영역에 같은 색상으로 분포된 것을 고려  \n",
    "  => mask5 파일명의 데이터의 경우 불규칙적인 마스크 디자인을 착용한 것을 확인\n",
    "  => 마스크의 색상 또한 다양한 색으로 보임(검정, 하양, 하늘색 등등)\n",
    "- 모든 사람들이 정면을 보고 정자세로 촬영한 것으로 사람의 얼굴 영역만 추출하면 좋을 것 같음\n",
    "  => 옷, 배경 등의 영향을 줄이기 위함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. DataSet 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.utils.data의 Dataset을 상속받아 Mask DataSet을 만들기.  \n",
    "- 주어진 데이터로부터 Labeling하기 (파일명 - 마스크착용상태 / Age - 나이범위 / gendar - 성별)\n",
    "- 가능한 Augmentation 하여 데이터 늘리기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. 주어진 데이터 Labeling하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/input/data/train/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  gender   race  age                    path\n",
       "0  000001  female  Asian   45  000001_female_Asian_45\n",
       "1  000002  female  Asian   52  000002_female_Asian_52\n",
       "2  000004    male  Asian   54    000004_male_Asian_54\n",
       "3  000005  female  Asian   58  000005_female_Asian_58\n",
       "4  000006  female  Asian   59  000006_female_Asian_59"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_dir)\n",
    "df = pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "      <th>age_range</th>\n",
       "      <th>full_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>between 30 and 60</td>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "      <td>between 30 and 60</td>\n",
       "      <td>/opt/ml/input/data/train/images/000002_female_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "      <td>between 30 and 60</td>\n",
       "      <td>/opt/ml/input/data/train/images/000004_male_As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "      <td>between 30 and 60</td>\n",
       "      <td>/opt/ml/input/data/train/images/000005_female_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "      <td>between 30 and 60</td>\n",
       "      <td>/opt/ml/input/data/train/images/000006_female_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  gender   race  age                    path          age_range  \\\n",
       "0  000001  female  Asian   45  000001_female_Asian_45  between 30 and 60   \n",
       "1  000002  female  Asian   52  000002_female_Asian_52  between 30 and 60   \n",
       "2  000004    male  Asian   54    000004_male_Asian_54  between 30 and 60   \n",
       "3  000005  female  Asian   58  000005_female_Asian_58  between 30 and 60   \n",
       "4  000006  female  Asian   59  000006_female_Asian_59  between 30 and 60   \n",
       "\n",
       "                                           full_path  \n",
       "0  /opt/ml/input/data/train/images/000001_female_...  \n",
       "1  /opt/ml/input/data/train/images/000002_female_...  \n",
       "2  /opt/ml/input/data/train/images/000004_male_As...  \n",
       "3  /opt/ml/input/data/train/images/000005_female_...  \n",
       "4  /opt/ml/input/data/train/images/000006_female_...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# age로 범위지정해서 새로운 column 만들기\n",
    "df['age_range'] = df['age'].apply(lambda x: 'younger then 30' if x < 30 else ('older then 60' if x >= 60 else 'between 30 and 60'))\n",
    "\n",
    "# fullpath\n",
    "df['full_path'] = df['path'].apply(lambda x: os.path.join(train_dir, 'images', x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2158\n",
      "542\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 817 732 109 549 410 83 : 2700\n",
    "# 163 146 22 110 82 17 : 540\n",
    "\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "df_t1, df_v1 = train_test_split(df.loc[df['gender'] == 'female'].loc[df['age_range'] == 'younger then 30'], test_size=VAL_SPLIT, random_state=42)\n",
    "df_t2, df_v2 = train_test_split(df.loc[df['gender'] == 'female'].loc[df['age_range'] == 'between 30 and 60'], test_size=VAL_SPLIT, random_state=42)\n",
    "df_t3, df_v3 = train_test_split(df.loc[df['gender'] == 'female'].loc[df['age_range'] == 'older then 60'], test_size=VAL_SPLIT, random_state=42)\n",
    "df_t4, df_v4 = train_test_split(df.loc[df['gender'] == 'male'].loc[df['age_range'] == 'younger then 30'], test_size=VAL_SPLIT, random_state=42)\n",
    "df_t5, df_v5 = train_test_split(df.loc[df['gender'] == 'male'].loc[df['age_range'] == 'between 30 and 60'], test_size=VAL_SPLIT, random_state=42)\n",
    "df_t6, df_v6 = train_test_split(df.loc[df['gender'] == 'male'].loc[df['age_range'] == 'older then 60'], test_size=VAL_SPLIT, random_state=42)\n",
    "\n",
    "train_df = pd.concat([df_t1, df_t2, df_t3, df_t4, df_t5, df_t6])\n",
    "validation_df = pd.concat([df_v1, df_v2, df_v3, df_v4, df_v5, df_v6])\n",
    "\n",
    "print(len(train_df))\n",
    "print(len(validation_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and validation set\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#VAL_SPLIT=0.2\n",
    "\n",
    "#train_df, validation_df = train_test_split(df, test_size=VAL_SPLIT, random_state=2021) # random_state is seed to fix data\n",
    "#print(f'train : {len(train_df)}, validation : {len(validation_df)}')\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e966eb655fb4be2b8a055cf0fc1870a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389104e1972d4f01b2219e9ff409fa50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train : 15106, validation : 3794\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class_dict = {\n",
    "    \"Wear\": {\"male\": {\"younger then 30\": \"0\", \"between 30 and 60\": \"1\", \"older then 60\": \"2\"}\n",
    "            ,\"female\": {\"younger then 30\": \"3\", \"between 30 and 60\": \"4\", \"older then 60\": \"5\"}},\n",
    "    \"incorrect\": {\"male\": {\"younger then 30\": \"6\", \"between 30 and 60\": \"7\", \"older then 60\": \"8\"}\n",
    "            ,\"female\": {\"younger then 30\": \"9\", \"between 30 and 60\": \"10\", \"older then 60\": \"11\"}},\n",
    "    \"Not Wear\": {\"male\": {\"younger then 30\": \"12\", \"between 30 and 60\": \"13\", \"older then 60\": \"14\"}\n",
    "            ,\"female\": {\"younger then 30\": \"15\", \"between 30 and 60\": \"16\", \"older then 60\": \"17\"}}\n",
    "}\n",
    "\n",
    "train_class_df = pd.DataFrame([], columns = ['gender', 'age_range', 'wearing_status', 'class', 'age', 'race', 'path', 'img_full_path', 'full_path'])\n",
    "validation_class_df = pd.DataFrame([], columns = ['class', 'img_full_path'])\n",
    "\n",
    "for index, row in tqdm(train_df.iterrows()):\n",
    "    img_dir_list = os.listdir(row.full_path)\n",
    "    img_dir_list = [img_file for img_file in img_dir_list if not img_file.startswith('.')] # remove hidden files\n",
    "    for img_file in img_dir_list:\n",
    "        row['img_full_path'] = os.path.join(row.full_path, img_file)\n",
    "        row['wearing_status'] = 'incorrect' if 'incorrect_mask' in img_file else ('Not Wear' if 'normal' in img_file else 'Wear')\n",
    "        row['class'] = class_dict[row.wearing_status][row.gender][row.age_range]\n",
    "        train_class_df = train_class_df.append(row, ignore_index=True)\n",
    "\n",
    "for index, row in tqdm(validation_df.iterrows()):\n",
    "    img_dir_list = os.listdir(row.full_path)\n",
    "    img_dir_list = [img_file for img_file in img_dir_list if not img_file.startswith('.')] # remove hidden files\n",
    "    for img_file in img_dir_list:\n",
    "        row['img_full_path'] = os.path.join(row.full_path, img_file)\n",
    "        row['wearing_status'] = 'incorrect' if 'incorrect_mask' in img_file else ('Not Wear' if 'normal' in img_file else 'Wear')\n",
    "        row['class'] = class_dict[row.wearing_status][row.gender][row.age_range]\n",
    "        validation_class_df = validation_class_df.append(row, ignore_index=True)        \n",
    "        \n",
    "# label 값을 위한 수치 Age : 0, 1, 2 / Gender : 0, 3 / Mask 0, 6, 12  프로젝트 소스로 옮길땐 이 방법으로 수정(더 효율적임)\n",
    "#df['age_point'] = df['age'].apply(lambda x: 0 if x < 30 else (2 if x >= 60 else 1))\n",
    "#df['gender_point'] = df['gender'].apply(lambda x: 0 if x.lower() == 'male' else 3)\n",
    "#row['wearing_status_point'] = 'incorrect' if 'incorrect_mask' in img_file else ('Not Wear' if 'normal' in img_file else 'Wear')        \n",
    "\n",
    "print(f'train : {len(train_class_df)}, validation : {len(validation_class_df)}') # 15120, 3780"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "between 30 and 60    4571\n",
      "younger then 30      4095\n",
      "older then 60         609\n",
      "Name: age_range, dtype: int64\n",
      "between 30 and 60    1148\n",
      "younger then 30      1029\n",
      "older then 60         154\n",
      "Name: age_range, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_class_df.head()\n",
    "print(train_class_df.loc[train_class_df['gender'] == 'female']['age_range'].value_counts())\n",
    "print(validation_class_df.loc[validation_class_df['gender'] == 'female']['age_range'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>img_full_path</th>\n",
       "      <th>age</th>\n",
       "      <th>age_range</th>\n",
       "      <th>full_path</th>\n",
       "      <th>gender</th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "      <th>race</th>\n",
       "      <th>wearing_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>/opt/ml/input/data/train/images/003093_female_...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>younger then 30</td>\n",
       "      <td>/opt/ml/input/data/train/images/003093_female_...</td>\n",
       "      <td>female</td>\n",
       "      <td>003093</td>\n",
       "      <td>003093_female_Asian_19</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>/opt/ml/input/data/train/images/003093_female_...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>younger then 30</td>\n",
       "      <td>/opt/ml/input/data/train/images/003093_female_...</td>\n",
       "      <td>female</td>\n",
       "      <td>003093</td>\n",
       "      <td>003093_female_Asian_19</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Not Wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>/opt/ml/input/data/train/images/003093_female_...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>younger then 30</td>\n",
       "      <td>/opt/ml/input/data/train/images/003093_female_...</td>\n",
       "      <td>female</td>\n",
       "      <td>003093</td>\n",
       "      <td>003093_female_Asian_19</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>/opt/ml/input/data/train/images/003093_female_...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>younger then 30</td>\n",
       "      <td>/opt/ml/input/data/train/images/003093_female_...</td>\n",
       "      <td>female</td>\n",
       "      <td>003093</td>\n",
       "      <td>003093_female_Asian_19</td>\n",
       "      <td>Asian</td>\n",
       "      <td>incorrect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>/opt/ml/input/data/train/images/003093_female_...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>younger then 30</td>\n",
       "      <td>/opt/ml/input/data/train/images/003093_female_...</td>\n",
       "      <td>female</td>\n",
       "      <td>003093</td>\n",
       "      <td>003093_female_Asian_19</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Wear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                      img_full_path   age  \\\n",
       "0     3  /opt/ml/input/data/train/images/003093_female_...  19.0   \n",
       "1    15  /opt/ml/input/data/train/images/003093_female_...  19.0   \n",
       "2     3  /opt/ml/input/data/train/images/003093_female_...  19.0   \n",
       "3     9  /opt/ml/input/data/train/images/003093_female_...  19.0   \n",
       "4     3  /opt/ml/input/data/train/images/003093_female_...  19.0   \n",
       "\n",
       "         age_range                                          full_path  gender  \\\n",
       "0  younger then 30  /opt/ml/input/data/train/images/003093_female_...  female   \n",
       "1  younger then 30  /opt/ml/input/data/train/images/003093_female_...  female   \n",
       "2  younger then 30  /opt/ml/input/data/train/images/003093_female_...  female   \n",
       "3  younger then 30  /opt/ml/input/data/train/images/003093_female_...  female   \n",
       "4  younger then 30  /opt/ml/input/data/train/images/003093_female_...  female   \n",
       "\n",
       "       id                    path   race wearing_status  \n",
       "0  003093  003093_female_Asian_19  Asian           Wear  \n",
       "1  003093  003093_female_Asian_19  Asian       Not Wear  \n",
       "2  003093  003093_female_Asian_19  Asian           Wear  \n",
       "3  003093  003093_female_Asian_19  Asian      incorrect  \n",
       "4  003093  003093_female_Asian_19  Asian           Wear  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_class_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 파일로 저장\n",
    "if os.path.isfile('./base_dataset/train_class_df.csv'):\n",
    "    os.remove('./base_dataset/train_class_df.csv')\n",
    "if os.path.isfile('./base_dataset/validation_class_df.csv'):\n",
    "    os.remove('./base_dataset/validation_class_df.csv')\n",
    "    \n",
    "train_class_df.to_csv('./base_dataset/train_class_df.csv')\n",
    "validation_class_df.to_csv('./base_dataset/validation_class_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################################################\n",
    "######################################재시작시 시작점###################################################################################################\n",
    "########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = {\n",
    "    'train': './base_dataset/train_class_df.csv',\n",
    "    'validation': './base_dataset/validation_class_df.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# SEED 를 고정\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "class Mask(Dataset):\n",
    "    def __init__(self, method, transform, train=True):\n",
    "        self.data = pd.read_csv(data_path[method])\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        \n",
    "        self.X = self.data['img_full_path']\n",
    "        self.y = self.data['class']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #print(self.X[idx])\n",
    "        img = cv2.imread(self.X[idx])\n",
    "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        face = cascade.detectMultiScale(rgb)\n",
    "        if len(face) == 0:\n",
    "            croped = rgb[100:400, 80:280]\n",
    "        else:\n",
    "            for x, y, w, h in face:\n",
    "                if w < 100:\n",
    "                    croped = rgb[100:400, 80:280]\n",
    "                    break\n",
    "                croped = rgb[max(0,y-50):min(y+h+20, 512), x:x+w]\n",
    "        \n",
    "        crop_img = Image.fromarray(croped)\n",
    "        crop_img = transforms.Resize((300,200))(crop_img)\n",
    "        \n",
    "        X = self.transform(crop_img)\n",
    "        y = self.y[idx]\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "        if not self.train:\n",
    "            X = torch.tensor(X, dtype=torch.float)\n",
    "            return X.clone().detach()\n",
    "            #return torch.tensor(X, dtype=torch.float)\n",
    "        #return torch.tensor(X, dtype=torch.float), torch.tensor(y, dtype=torch.long)\n",
    "        return X.clone().detach(), y.clone().detach()\n",
    "\n",
    "#         img = Image.open(self.X[idx])\n",
    "        \n",
    "#         X = self.transform(img)\n",
    "#         y = self.y[idx]\n",
    "\n",
    "#         X = torch.tensor(X, dtype=torch.float)\n",
    "#         y = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "#         if not self.train:\n",
    "#             X = torch.tensor(X, dtype=torch.float)\n",
    "#             return X.clone().detach()\n",
    "#             #return torch.tensor(X, dtype=torch.float)\n",
    "#         #return torch.tensor(X, dtype=torch.float), torch.tensor(y, dtype=torch.long)\n",
    "#         return X.clone().detach(), y.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Mask(method='train', transform=transform, train=True)\n",
    "validation_dataset = Mask(method='validation', transform=transform, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. DataLoader 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "seed_everything(seed)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/input/data/train/images/003070_female_Asian_20/mask3.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-42cb521d51ab>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.float)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6765,  0.6765,  0.6765,  ...,  0.0882,  0.1667,  0.3627],\n",
       "          [ 0.6765,  0.6765,  0.6765,  ...,  0.1078,  0.1667,  0.2255],\n",
       "          [ 0.6765,  0.6765,  0.6765,  ...,  0.2059,  0.2255,  0.1275],\n",
       "          ...,\n",
       "          [-2.3039, -2.2843, -2.2647,  ..., -1.4804, -1.4804, -1.4804],\n",
       "          [-2.3039, -2.2843, -2.2647,  ..., -1.4804, -1.4804, -1.4804],\n",
       "          [-2.3039, -2.2843, -2.2451,  ..., -1.4804, -1.4804, -1.4804]],\n",
       " \n",
       "         [[ 0.6569,  0.6569,  0.6569,  ...,  0.0686,  0.1471,  0.3431],\n",
       "          [ 0.6569,  0.6569,  0.6569,  ...,  0.0882,  0.1471,  0.2059],\n",
       "          [ 0.6569,  0.6569,  0.6569,  ...,  0.1863,  0.2059,  0.1078],\n",
       "          ...,\n",
       "          [-2.3039, -2.2843, -2.2647,  ..., -1.6176, -1.6176, -1.6176],\n",
       "          [-2.3039, -2.2843, -2.2647,  ..., -1.6176, -1.6176, -1.6176],\n",
       "          [-2.3039, -2.2843, -2.2843,  ..., -1.6176, -1.6176, -1.6176]],\n",
       " \n",
       "         [[ 0.5784,  0.5784,  0.5784,  ..., -0.0294,  0.0490,  0.2451],\n",
       "          [ 0.5784,  0.5784,  0.5784,  ..., -0.0098,  0.0490,  0.1078],\n",
       "          [ 0.5784,  0.5784,  0.5784,  ...,  0.0882,  0.1078,  0.0098],\n",
       "          ...,\n",
       "          [-2.2647, -2.2451, -2.2255,  ..., -1.9314, -1.9314, -1.9314],\n",
       "          [-2.3039, -2.2843, -2.2647,  ..., -1.9706, -1.9706, -1.9706],\n",
       "          [-2.3039, -2.2843, -2.2647,  ..., -1.9706, -1.9706, -1.9706]]]),\n",
       " tensor(3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[11]\n",
    "# sample_img, sample_label = next(iter(train_loader))\n",
    "# sample_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Model 만들기(가져오기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0282, -0.0293,  0.0232, -0.0018, -0.0251, -0.0441,  0.0382,  0.0027,\n",
       "        -0.0391, -0.0218, -0.0271,  0.0262,  0.0390,  0.0339, -0.0274, -0.0178,\n",
       "         0.0161,  0.0222])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torchvision\n",
    "\n",
    "seed_everything(seed)\n",
    "mask_resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "mask_resnet18.fc = torch.nn.Linear(in_features=512, out_features=18, bias=True)\n",
    "torch.nn.init.xavier_uniform_(mask_resnet18.fc.weight)\n",
    "stdv = 1. / math.sqrt(mask_resnet18.fc.weight.size(1))\n",
    "mask_resnet18.fc.bias.data.uniform_(-stdv, stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 is using!\n"
     ]
    }
   ],
   "source": [
    "seed_everything(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # 학습 때 GPU 사용여부 결정. Colab에서는 \"런타임\"->\"런타임 유형 변경\"에서 \"GPU\"를 선택할 수 있음\n",
    "print(f\"{device} is using!\")\n",
    "mask_resnet18.to(device) # Resnent 18 네트워크의 Tensor들을 GPU에 올릴지 Memory에 올릴지 결정함\n",
    "\n",
    "LEARNING_RATE = 0.0001 # 학습 때 사용하는 optimizer의 학습률 옵션 설정\n",
    "NUM_EPOCH = 10 # 학습 때 mnist train 데이터 셋을 얼마나 많이 학습할지 결정하는 옵션\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss() # 분류 학습 때 많이 사용되는 Cross entropy loss를 objective function으로 사용 - https://en.wikipedia.org/wiki/Cross_entropy\n",
    "optimizer = torch.optim.Adam(mask_resnet18.parameters(), lr=LEARNING_RATE) # weight 업데이트를 위한 optimizer를 Adam으로 사용함\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0, last_epoch=-1, verbose=False)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\" : train_loader,\n",
    "    \"validation\" : validation_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. wandb 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlswkim\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">twilight-plant-25</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/team-34/image-classification-mask\" target=\"_blank\">https://wandb.ai/team-34/image-classification-mask</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/team-34/image-classification-mask/runs/20wwscwz\" target=\"_blank\">https://wandb.ai/team-34/image-classification-mask/runs/20wwscwz</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/code/wandb/run-20210829_205004-20wwscwz</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb 세팅\n",
    "import wandb\n",
    "\n",
    "#NUM_EPOCH = 5 # 그냥 한번 더 씀 (보기 쉽게)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "VAL_SPLIT = 0.2\n",
    "OPTIMIZER = 'ADAM'\n",
    "SCHEDULAR = 'CosineAnnealingLR'\n",
    "AUGMENTATION = 'None'\n",
    "\n",
    "config = {\n",
    "    'epochs': NUM_EPOCH, 'batch_size': BATCH_SIZE, 'learning_rate': LEARNING_RATE,\n",
    "    'val_split': VAL_SPLIT, 'Schedular': SCHEDULAR,  'Augmentation': AUGMENTATION\n",
    "}\n",
    "\n",
    "wandb.init(project='image-classification-mask', entity='team-34', config=config)\n",
    "wandb.run.name = 'ksw_4th_resNet18' # 회차 이름 명명 규칙 필요할 것 같음. {이름}_{회차}_{모델명} \n",
    "wandb.watch(mask_resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Training 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e6d537152b44b18cb45e779f4fe04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-0의 train-데이터 셋에서 평균 Loss : 0.522, 평균 Accuracy : 0.840\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bcc95537e83415291c03872b9a092cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-0의 validation-데이터 셋에서 평균 Loss : 0.402, 평균 Accuracy : 0.873\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153bddbd92994d46b7930c89649afbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-1의 train-데이터 셋에서 평균 Loss : 0.161, 평균 Accuracy : 0.950\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a959c4d840974646920a3d3362497b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-1의 validation-데이터 셋에서 평균 Loss : 0.415, 평균 Accuracy : 0.880\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b261505523f41a3b02ab0f0834807e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-2의 train-데이터 셋에서 평균 Loss : 0.069, 평균 Accuracy : 0.981\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512ed71d7b8f4312a3d6dec821c7da31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-2의 validation-데이터 셋에서 평균 Loss : 0.671, 평균 Accuracy : 0.843\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bf0b598a804692a881e4c8f5bc40f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-3의 train-데이터 셋에서 평균 Loss : 0.042, 평균 Accuracy : 0.990\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f497154c354f23a9ba907f3398ff05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-3의 validation-데이터 셋에서 평균 Loss : 0.471, 평균 Accuracy : 0.876\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e37c8872f74f26a74ffd8649e4fb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-4의 train-데이터 셋에서 평균 Loss : 0.031, 평균 Accuracy : 0.993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016af841559d4fa8896d2bb47016ad61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-4의 validation-데이터 셋에서 평균 Loss : 0.537, 평균 Accuracy : 0.882\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb24a072cdc4489897ae2cb2ad556935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-5의 train-데이터 셋에서 평균 Loss : 0.028, 평균 Accuracy : 0.993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d73043302144d2a898f82701358417d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-5의 validation-데이터 셋에서 평균 Loss : 0.533, 평균 Accuracy : 0.878\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0aeed00b5cb4c519475453c2f0efff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-6의 train-데이터 셋에서 평균 Loss : 0.014, 평균 Accuracy : 0.997\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac6da8431f447d1a5bd52725882b07a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-6의 validation-데이터 셋에서 평균 Loss : 0.496, 평균 Accuracy : 0.871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c6ec8623cd4f66b97d3bca67249007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-7의 train-데이터 셋에서 평균 Loss : 0.026, 평균 Accuracy : 0.993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749466747d834d1b9e6987d5a28cbd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-7의 validation-데이터 셋에서 평균 Loss : 0.591, 평균 Accuracy : 0.877\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f198a90001974ec2a05b4b2f97a324d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-8의 train-데이터 셋에서 평균 Loss : 0.076, 평균 Accuracy : 0.974\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0e18cd3efa41109eb0ed27080c0884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-8의 validation-데이터 셋에서 평균 Loss : 0.574, 평균 Accuracy : 0.872\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5c8c59f0db4d1c81625dc9b803c8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-9의 train-데이터 셋에서 평균 Loss : 0.015, 평균 Accuracy : 0.996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0378918cf944e5f807ce47898467397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-9의 validation-데이터 셋에서 평균 Loss : 0.492, 평균 Accuracy : 0.879\n",
      "학습 종료!\n",
      "최고 accuracy : 0.8819188475608826, 최고 낮은 loss : 0.4023524811967587\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8861<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/opt/ml/code/wandb/run-20210829_205004-20wwscwz/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/opt/ml/code/wandb/run-20210829_205004-20wwscwz/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train loss</td><td>0.01501</td></tr><tr><td>train acc</td><td>0.9959</td></tr><tr><td>train f1</td><td>0.99342</td></tr><tr><td>_runtime</td><td>19281</td></tr><tr><td>_timestamp</td><td>1630289485</td></tr><tr><td>_step</td><td>19</td></tr><tr><td>validation loss</td><td>0.49231</td></tr><tr><td>validation acc</td><td>0.87876</td></tr><tr><td>validation f1</td><td>0.64586</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train loss</td><td>█▃▂▁▁▁▁▁▂▁</td></tr><tr><td>train acc</td><td>▁▆▇█████▇█</td></tr><tr><td>train f1</td><td>▁▅▇█████▇█</td></tr><tr><td>_runtime</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██</td></tr><tr><td>_timestamp</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██</td></tr><tr><td>_step</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>validation loss</td><td>▁▁█▃▅▄▃▆▅▃</td></tr><tr><td>validation acc</td><td>▆█▁▇█▇▆▇▆▇</td></tr><tr><td>validation f1</td><td>▆▆▁▅█▆▃▅▅▃</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">twilight-plant-25</strong>: <a href=\"https://wandb.ai/team-34/image-classification-mask/runs/20wwscwz\" target=\"_blank\">https://wandb.ai/team-34/image-classification-mask/runs/20wwscwz</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "### 학습 코드 시작\n",
    "best_test_accuracy = 0.\n",
    "best_test_loss = 9999.\n",
    "seed_everything(seed)\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    for phase in [\"train\", \"validation\"]:\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        running_f1 = 0.\n",
    "        \n",
    "        if phase == \"train\":\n",
    "            mask_resnet18.train() # 네트워크 모델을 train 모드로 두어 gradient을 계산하고, 여러 sub module (배치 정규화, 드롭아웃 등)이 train mode로 작동할 수 있도록 함\n",
    "        elif phase == \"validation\":\n",
    "            mask_resnet18.eval() # 네트워크 모델을 eval 모드 두어 여러 sub module들이 eval mode로 작동할 수 있게 함\n",
    "\n",
    "        n_iter = 0\n",
    "        for ind, (images, labels) in enumerate(tqdm(dataloaders[phase])):\n",
    "            # (참고.해보기) 현재 tqdm으로 출력되는 것이 단순히 진행 상황 뿐인데 현재 epoch, running_loss와 running_acc을 출력하려면 어떻게 할 수 있는지 tqdm 문서를 보고 해봅시다!\n",
    "            # hint - with, pbar\n",
    "            #print(f'ind : {ind}')\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad() # parameter gradient를 업데이트 전 초기화함\n",
    "\n",
    "            with torch.set_grad_enabled(phase == \"train\"): # train 모드일 시에는 gradient를 계산하고, 아닐 때는 gradient를 계산하지 않아 연산량 최소화\n",
    "                logits = mask_resnet18(images)\n",
    "                _, preds = torch.max(logits, 1) # 모델에서 linear 값으로 나오는 예측 값 ([0.9,1.2, 3.2,0.1,-0.1,...])을 최대 output index를 찾아 예측 레이블([2])로 변경함  \n",
    "                loss = loss_fn(logits, labels)\n",
    "\n",
    "            if phase == \"train\":\n",
    "                loss.backward() # 모델의 예측 값과 실제 값의 CrossEntropy 차이를 통해 gradient 계산\n",
    "                optimizer.step() # 계산된 gradient를 가지고 모델 업데이트\n",
    "                scheduler.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0) # 한 Batch에서의 loss 값 저장\n",
    "            running_acc += torch.sum(preds == labels.data) # 한 Batch에서의 Accuracy 값 저장\n",
    "            running_f1 += f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
    "            \n",
    "            n_iter += 1\n",
    "\n",
    "        # 한 epoch이 모두 종료되었을 때,\n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        epoch_acc = running_acc / len(dataloaders[phase].dataset)\n",
    "        epoch_f1 = running_f1 / n_iter\n",
    "\n",
    "        print(f\"현재 epoch-{epoch}의 {phase}-데이터 셋에서 평균 Loss : {epoch_loss:.3f}, 평균 Accuracy : {epoch_acc:.3f}\")\n",
    "\n",
    "        wandb.log({\n",
    "                f'{phase} loss': epoch_loss,\n",
    "                f'{phase} acc': epoch_acc,\n",
    "                f'{phase} f1': epoch_f1,\n",
    "        })\n",
    "        \n",
    "        if phase == \"validation\" and best_test_accuracy < epoch_acc: # phase가 test일 때, best accuracy 계산\n",
    "            best_test_accuracy = epoch_acc\n",
    "        if phase == \"validation\" and best_test_loss > epoch_loss: # phase가 test일 때, best loss 계산\n",
    "            best_test_loss = epoch_loss\n",
    "            \n",
    "print(\"학습 종료!\")\n",
    "print(f\"최고 accuracy : {best_test_accuracy}, 최고 낮은 loss : {best_test_loss}\")\n",
    "wandb.finish()\n",
    "\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "save_name = f\"{dt.now().strftime('%Y%m%d%H%M')}_mask.pt\"\n",
    "torch.save(mask_resnet18, os.path.join(save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class 0     is: 93.5 %\n",
      "Accuracy for class 1     is: 79.5 %\n",
      "Accuracy for class 2     is: 69.4 %\n",
      "Accuracy for class 3     is: 93.1 %\n",
      "Accuracy for class 4     is: 90.0 %\n",
      "Accuracy for class 5     is: 64.5 %\n",
      "Accuracy for class 6     is: 94.5 %\n",
      "Accuracy for class 7     is: 82.9 %\n",
      "Accuracy for class 8     is: 64.7 %\n",
      "Accuracy for class 9     is: 88.4 %\n",
      "Accuracy for class 10    is: 90.2 %\n",
      "Accuracy for class 11    is: 54.5 %\n",
      "Accuracy for class 12    is: 93.6 %\n",
      "Accuracy for class 13    is: 85.4 %\n",
      "Accuracy for class 14    is: 58.8 %\n",
      "Accuracy for class 15    is: 89.8 %\n",
      "Accuracy for class 16    is: 83.5 %\n",
      "Accuracy for class 17    is: 54.5 %\n"
     ]
    }
   ],
   "source": [
    "classes = [str(num) for num in range(18)]\n",
    "\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in validation_loader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = mask_resnet18(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
